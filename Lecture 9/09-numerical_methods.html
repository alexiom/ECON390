<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economists</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alex Marsh" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Science for Economists
## Lecture 9: Intro to Numerical Methods
### Alex Marsh
### University of North Carolina | <a href="https://github.com/alexiom/ECON390">ECON 390</a>

---

name: toc

&lt;style type="text/css"&gt;
@media print {
  .has-continuation {
    display: block !important;
  }
}
&lt;/style&gt;



# Table of contents

1. [Introduction](#introduction)

2. [Math and Stat Review](#review)

3. [Intro to Nunmerical Methods](#num methods)

---
class: inverse, center, middle
name: prologue

# Introduction

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

---


# Agenda

Today we will cover numerical methods and optimization.

This lecture might be a bit more theory heavy than programming heavy. 

What is important here is to understand the concepts rather than the details.

- Unless you specialize in computational methods for economics later on, you will likely not need to understand the details.

---
# Motivation

- There are many times there is an equation we would like to solve or know the value of.
- While ideally, we would solve these analytically to get an exact solution, for any non-trivial problem, this is either incredibly tedious or not possible.
- Today we will cover a handful of methods that you might need at some point.
- While some topics will relate within this lecture, most likely, they will feel disjoint and spattered.
- Understand that I am teaching a handful of tools that you will likely need at some point.

---
class: inverse, center, middle
name: intro

# Introduction

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;
---

class: inverse, center, middle
name: review

# Review

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;

(Some important math and stat concepts)

---
# Math Review: Derivative

- The derivative of a function tells you about it's rate of change.
 - "Instantaneous slope" (this is actually nonsensical the more one thinks about it)
- We denote the derivative of `\(f\)` with respect to `\(x\)` a few different ways:
 - `\(f'(x)\)`
 - `\(\frac{df}{dx}\)`
- The derivative tells us more than just the slope:
 - `\(f'(x)&gt;0 \rightarrow\)` function is increasing 
 - `\(f'(x)&lt;0 \rightarrow\)` function is decreasing 
 - `\(f'(x)=0 \rightarrow\)` function is not changing i.e. "critical point" 
- The formal definition of a derivative is as follows:
`$$f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}$$`
- We will actually be using this definition!


---
# Math Review: Integrals

- Integrals calculate the area "under the curve"
- More broadly, they are used when you want to "sum up" a lot of very small things or take the average value of some function.
- The formal definition of the integral is `$$\int_{a}^{b}f(x)dx = \lim_{n\rightarrow \infty} \sum_{i=1}^{n}f(x_i) \Delta x$$` 
where `\(a\leq x_1 &lt; x_2 &lt; ...&lt; x_n\leq b\)` and `\(x_{i+1}-x_{i}=\Delta x = (b-a)/n\)`
- We will be using this definition!

---
class: inverse, center, middle
name: num methods

# Intro to Numerical Methods

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;


---
# Approximating Derivatives

- While getting the analytical expression for `\(f'(x)\)` is ideal due to accuracy and computational demand, sometimes it is not feasible to do so.
- Instead, we can approximate the derivative!
- Remember that 
`$$f'(x)=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}$$`.
- So for a "small" `\(h&gt;0\)`, 
`$$f'(x)\approx \frac{f(x+h)-f(x)}{h}$$`
- This is the "forward differencing" approach; we can also "backward difference"
`$$f'(x)\approx \frac{f(x)-f(x-h)}{h}$$`
- These works, but we can get a more accurate approximation for a fixed `\(h\)`.

---
# Approximating Derivatives (Cont.)

- Instead of forward or backward differencing, we can center difference:
`$$f'(x)\approx \frac{f(x+h)-f(x-h)}{2h}$$`
- Instead of favoring one side, we are approximating the derivative equally around a "neighborhood" of `\(x\)`.
- It is possible to show that the error generator by center differencing is smaller than right or left differencing.
- There are even more accurate methods that a bit more computationally demanding.
  - For those curious, look up Richardson's Extrapolation Method.

---
# Example

Let's keep things simple and look at the derivative of `\(f(x)=x^2\)`. The analytical expression of the derivative is `\(f'(x) = 2x\)`.


```r
f            = function(x){x^2}        #make function f(x)
fp           = function(x){2*x}        #make f'(x)
xs           = seq(0,2,0.5)            #store x's
h            = 0.01                    #store step size
der          = fp(xs)                  #calc actual derivatives
center_der   = (f(xs+h)-f(xs-h))/(2*h) #calc center differenced
forward_der  = (f(xs+h)-f(xs))/(h)     #calc forward differenced
backward_der = (f(xs)-f(xs-h))/(h)     #calc backwards differenced
deriv_data   = data.table("f'(x)"=der,"Center"=center_der,
                        "Foward"=forward_der,"Backward"=backward_der)
deriv_data
```

```
##    f'(x) Center Foward Backward
## 1:     0      0   0.01    -0.01
## 2:     1      1   1.01     0.99
## 3:     2      2   2.01     1.99
## 4:     3      3   3.01     2.99
## 5:     4      4   4.01     3.99
```


---
# Derivatives in Multiple Dimensions

- If instead of `\(f(x)\)`, we have something like `\(f(x_1,x_2,...,x_n)\)`, not much changes.
- The derivative will be a vector called the gradient denoted 

`$$\nabla_f = \Big(\frac{\partial f}{\partial x_1},...,\frac{\partial f}{\partial x_n}\Big)$$`

- To approximate each (partial) derivative, follow the same approach.
- The partial derivative of `\(f\)` in the `\(x_i\)` dimension at `\(x = (x_1,...,x_n)\)` is,

$$\frac{\partial f}{\partial x_i} \approx \frac{f(x_1,...,x_i+h,...,x_n)-f(x_1,...,x_i-h,...,x_n)}{2h} $$
- Do this for all inputs and then collect them in a vector.
- This requires evaluating the function `\(2n\)` times.

---
# Example

Suppose that `\(f(x,y) = x^2 + \text{sin}(y)\)`. `\(\frac{\partial f}{\partial x} = 2x\)` and `\(\frac{\partial f}{\partial y} = \text{cos}(y)\)`


```r
f            = function(x){x[1]^2+sin(x[2])}        #make function f(x)
fp           = function(x){c(2*x[1],cos(x[2]))}     #make gradient of f(x)
h            = 0.01                                 #store step size
fp(c(1,0.5))
```

```
## [1] 2.0000000 0.8775826
```

```r
c((f(c(1+h,0.5))-f(c(1-h,0.5)))/(2*h),(f(c(1,0.5+h))-f(c(1,0.5-h)))/(2*h))
```

```
## [1] 2.0000000 0.8775679
```

```r
c((f(c(1+h,0.5))-f(c(1,0.5)))/h,(f(c(1,0.5+h))-f(c(1,0.5)))/h)
```

```
## [1] 2.0100000 0.8751708
```

```r
c((f(c(1,0.5))-f(c(1-h,0.5)))/h,(f(c(1,0.5))-f(c(1,0.5-h)))/h)
```

```
## [1] 1.990000 0.879965
```

---

# Approximating Integrals

- Just like derivatives, sometimes we might need to approximate an integral.
- Unlike derivatives, sometimes integrals don't have a closed-form expression at all!
- Therefore, all we can get for integrals is a numerical approximation.
- Remember that 
`$$\int_{a}^{b}f(x)dx = \lim_{n\rightarrow \infty} \sum_{i=1}^{n}f(x_i) \Delta x$$` 
- So for certain "partition" of points, `\(a\leq x_1 &lt; x_2 &lt; ...&lt; x_n\leq b\)` and `\(x_{i+1}-x_{i}=\Delta x = (b-a)/n\)`,

`$$\int_{a}^{b}f(x)dx \approx  \sum_{i=1}^{n}f(x_i) \Delta x$$` 

- Note that a larger `\(n\)` makes this more accurate but means `\(f\)` must be evaluated `\(n\)`-times.
- Also note that depending on if we include `\(a\)` or `\(b\)` as a point makes this an upper sum or a lower sum.
  - Upper sums overestimate the integral.
  - Lower sums underestimate the integral.
  - Which to choose?
---
#Upper sums and lower sums

![](pics/riemann-sums-area-distances.png)

---
# The Trapezoidal Rule

- We don't need to pick upper or lower sums, we can use both! Sorta.
- Instead of rectangles, we use trapezoids.

&lt;img src="pics/trapezoidal_rule.png" width="75%" style="display: block; margin: auto;" /&gt;

---
# The Trapezoidal Rule

- Each of the trapezoids is `\(\frac{a+b}{2}h\)`
- Here `\(h = \Delta x_i\)`, `\(a=f(x_{i-1})\)`, `\(b=f(x_{i})\)`, `\(\Delta x_i=x_{i}-x_{i-1}\)`.
- So the area of each trapezoid is `\(\frac{f(x_{i-1})+f(x_{i1})}{2}\Delta x_i\)`
- Therefore, we know

`$$\int_{a}^{b}f(x)dx \approx \sum_{i=1}^{n} \frac{f(x_{i-1})+f(x_{i})}{2}\Delta x_i$$`
- The above expression works for an arbitrary grid i.e. `\(\Delta x_i\)` need not equal `\(\Delta x_{i+k}\)`
- If the grid is uniform, the above becomes a simple expression:

`$$\sum_{i=1}^{n} \frac{f(x_{i-1})+f(x_{i})}{2}\Delta x_i = \frac{\Delta x}{2}(f(x_0)+2f(x_1)+...+2f(x_{n-1})+f(x_n))$$`
- This approximation is more accurate than the upper or lower sums.

---
# Example

Suppose `\(f(x)=x^2\)` and we want to evaluate `\(\int_{0}^{1.5}f(x)dx\)`.


```r
f            = function(x){x^2}        #make function f(x)
Fx           = function(x){x^3/3}
trap_rule = function(func,a,b,n=20){
  dx = (b-a)/n
  xs = seq(a,b,dx)
  val = sum(2*func(xs[-c(1,n)]))
  val = val + func(xs[1])+func(xs[n])
  val*dx/2
}
true_val = Fx(1.5)-Fx(0)
my_approx_val20 = trap_rule(f,0,1.5,n=20)
my_approx_val100 = trap_rule(f,0,1.5,n=100)
approx_val = integrate(f,0,1.5)[[1]]
c(true_val,my_approx_val20,my_approx_val100,approx_val)
```

```
## [1] 1.125000 1.134633 1.125392 1.125000
```

---
# Multivariate Integration
- Unlike derivatives, multidimensional integration becomes difficult quickly. 
- One approach to multidimensional integration is actually Monte Carlo simulation, which we covered last lecture.
- A large class of multidimensional integration needed for probability distributions can be evaluated using Markov Chain Monte Carlo (MCMC) techniques.
 - Some examples include the Metropolis-Hastings algorithm and Gibbs Sampling.
 - These are needed for Bayesian inference.
 - Beyond the scope of this class, but worth looking into on your own.

---
# Solving for Roots

- Many times you'll want to solve an equation numerically.
  - This may because the closed-form solution is impossible or just tedious.
- When this is, we say we want to find the roots (i.e. `\(x\)` values) s.t. 
`$$f(x)=0$$`
- While we will cover the simplest methods to do so, the important thing to remember is the motivation and when you might want to use such techniques.
  - Better programmers than you have programmed faster and more robust methods.
- Dates back to the Babylonians and calculating square roots:

`$$\sqrt{a} = x$$`
or 
`$$x^2 - a = 0$$`  
---
# Bisection Method

- One of the two methods we will cover here is the bisection method.
- While this method will always work, it can be slow.
- In order to use the method, you must have starting values `\(a\)` and `\(b\)` where

`$$f(a) &lt; 0 &lt; f(b) \text{ or } f(a) &gt; 0 &gt; f(b)$$`
- The idea is we take the midpoint between `\(a\)` and `\(b\)`, which we cal `\(c\)`.
- If `\(\text{sign}(f(c))=\text{sign}(f(a))\)`, replace `\(a\)` with `\(c\)`, otherwise `\(b\)` with `\(c\)`.
- Continue until `\(|f(c)|&lt;\varepsilon\)` where `\(\varepsilon\)` is a tolerance value.

---
# Bisection Method

&lt;img src="pics/Bisection_method.png" width="50%" style="display: block; margin: auto;" /&gt;

---
# Reflection

- The bisection method works, but it is slow and sorta fancy "guess and check."
- We are only using the sign of the function to update the guesses.
- If we could incorporate more information of the function into the search process, it might be faster.
- Idea: Can we use the derivative of the function to tell us where to go next?
- This is known as Newton's method and it was the first improvement 

---
# Newton's Method
- Equation of tangent line in point-slope form at `\(x\)`:
`$$f(x) - f(x_n) = f'(x_n)(x-x_n)$$`
- Set `\(f(x)=0\)`, replace `\(x\)` with `\(x_{n+1}\)`, and solve for `\(x\)`:
`$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$`
- So make an initial guess `\(x_0\)`, form `\(x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}\)`.
- Keep updating `\(x_{n+1}\)` until `\(|f(x_{n+1})|&lt;\varepsilon\)`
- Question: What technique that we learned earlier will most likely be needed for this method?

---
# Comparing Methods




```r
f = function(x){x^2-2}
bisect_results = biscetion(f,0,2)
newton_results = Newtons(f,2)
bisect_results
```

```
## [1]  1.414214 47.000000
```

```r
newton_results
```

```
## [1] 1.414214 5.000000
```

---
# Fixed Points
- Many times in economics, we are interested in finding a fixed point, which is defined as 
`$$f(x) = x$$`
or
`$$f(x) - x = 0$$`
- Example: Supply and Demand equlibirum can be written as
`$$P^S(Q^d(p)))=p$$`
- At it's core, there is nothing different for solving fixed points.
 - Define `\(g(x) = f(x) - x\)` and solve `\(g(x)=0\)`.
- However, for some special functions, you can solve `\(f(x)=x\)` via fixed point iteration.
- If the function `\(f\)` is a contraction, then we can simply iterate
  - `\(x_1 = f(x_0)\)`
  - `\(x_2 = f(x_1)\)`
  - `\(x_{n+1} = f(x_n)\)`
- What is a contraction?

---
# Contractions
- Example: `\(f(x)=\text{cos}(x)\)`

```r
x0 = seq(0,2*pi,0.25)
xn = x0
for(n in 1:100){
  xnp1 = cos(xn)
  xn   = xnp1
}
xnp1
```

```
##  [1] 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851
##  [8] 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851
## [15] 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851
## [22] 0.7390851 0.7390851 0.7390851 0.7390851 0.7390851
```
- Unfortunately, most functions are not contractions. 
- However, there are some mathematical objects that are universally contractions and if you go onto graduate studies, you will experience them.
 - Dynamic programming and value functions.
 
---
# Multivariate Root Finding
- You might need to find 
$$f_i (x_1,...,x_n) = 0 \text{ for } i = 1,...,n $$
- There are methods similar to the ones we have already discussed for solving these problems.
- However, they are much outside the scope of this class. 
- For now, it is sufficient to know that there exists a package in `R` for solving these problems: `nleqslv`.
- While the algorithms are more complex, the ideas remain the same.
- You will need `nleqslv` for the problem set.
 
---
# Optimization
- The last numerical methods topic that we will cover is optimization.
- An optimization problem is one that can be written as follows:
$$ \min_{x} f(x)$$
- Note, if we want to maximize, that's the same as minimizing `\(-f(x)\)`.
- We want to find the `\(x\)` value that minimizes `\(f(x)\)`.
- Sometimes, these will result in closed form solutions (e.g. OLS), but usually will be oto complicated.
- We write the solution to this optimization problem as
`$$x^* = \text{arg}\min_{x} f(x)$$`
- For functions that are differentiable, they are optimized at `\(f'(x)=0\)`.
  - Look familiar?
- This is just root finding of `\(g(x)=0\)` where `\(f'(x)=g(x)\)`.

---
# General Issues That Araise
- The first big issue that arises is that maxs, mins, and saddle points all have `\(f'(x)=0\)`.
- Also, there can be local minimums if the function is not globally convex.
- This makes numerical optimization difficult!
- Using the second derivative/Hessian matrix can help solve some of these issues, but it is very computationally expensive to compute.
- As such, no one technique will always work.
- Lots of time and research goes into figuring out the best way to optimize one specific optimization problem.
 - E.g. check out this [best practices paper](https://chrisconlon.github.io/site/pyblp.pdf) for one model used in my subfield.
- Sometimes an entire project can live or die depending upon if you can optimize the objective function correctly.
 - E.g. one of my current projects...

---
# Grid Search

- If the dimension of `\(x\)` is small enough, you can make a grid of points to search on and see which set of values minimizes `\(f\)`.
- While this seems simple, in practice, you rarely want to do it. 
- If `\(x\)` has more than two dimensions, must search many points.
 - Particularly bad of `\(f\)` takes awhile to run.
- Must make grid fine enough so that you're not skipping over too many points, but too fine runs into the same problem as before.


```r
x1 = seq(0,5,0.05)
x2 = seq(0,5,0.05)
x3 = seq(0,5,0.05)
nrow(expand.grid(x1,x2))
```

```
## [1] 10201
```

```r
nrow(expand.grid(x1,x2,x3))
```

```
## [1] 1030301
```
---

# Newton's Method... Again!

- We can actually use Newton's Method to solve optimization problems.
- Since we want to solve `\(f'(x) = 0\)` and using `\(g(x)=f'(x)\)`, apply the formula from before:
`$$x_{n+1} = x_n - \frac{g(x_n)}{g'(x_n)}$$`
or
`$$x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}$$`
- Note that we must compute a derivative and a second derivative for each iteration.
- Computing the derivative requires evaluating the function twice.
- Computing the second derivative requires evaluating the function four times.
- Generally, if you have `\(n\)` inputs, you have to evaluate the function `\(4n^2/2\)` times for the Hessian.
- So while this works, if your objective function takes awhile to run or has a lot of inputs, it might not be ideal.
 - Usually not ideal...

---
# Gradient Descent
- All the second derivative does is scale the updating process so we don't "learn" too fast or too slow. 
- Idea: Instead of calculating the scaling amount via the second derivative, we just set some paramter `\(\alpha&gt;0\)` to update
`$$x_{n+1} = x_n - \alpha f'(x_n)$$`
- This is the idea behind gradient descent. 
- Choosing `\(\alpha\)`:
 - Sometimes `\(\alpha\)` is just fixed at some small value.
 - However, you can also pick an optimal `\(\alpha\)`:
 1. Calculate `\(f'(x_n)\)` and save it.
 2. Then choose `\(\alpha\)` to minimize `\(f(x_n - \alpha f'(x_n))\)`.
 3. Repeat this step each time you update `\(x_{n+1}\)`.
 - Whether this is beneficial is problem specific.
 - You have to solve a smaller optimization problem during your larger optimization problem.
 
---
# Other Methods

- There are other methods.
 - Quasi-Newton: Calculate a function `\(B(x_n)\)` that approximates the Hessian but is easier to calculate.
 - Derivative free methods:
 1. Nelder-Mead
 2. Simulated annealing
 3. BOBYQA,COBYLA
 
- Which method to choose?
 - Problem specific!
 - Depends on the properties of `\(f\)`, how long it takes `\(f\)` to run, how many inputs `\(f\)` has, etc.
 - Unfortunately, there is no one answer.
 - Optimization can make or break a project.

---
# Other Methods

- Robustness: Performs well for various problems and starting values.
- Efficiency: Achieves the solution relatively quickly.
- Accuracy: Identify a solution with precision, not sensitive to starting values.

---
class: inverse, center, middle

# Up Next: Data Wrangling

&lt;html&gt;&lt;div style='float:left'&gt;&lt;/div&gt;&lt;hr color='#EB811B' size=1px width=796px&gt;&lt;/html&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
